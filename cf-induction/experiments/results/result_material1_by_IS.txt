
#A: Ration of training examples to be used 
#   from the examples except for the test example 
#   (we test the predictive accuracy with a leave-one-out strategy) 
#B: Dualization time [msec] 
#   (The executing time for computing the bottom theory)
#C: Generalization time [msec]
#   (The executing time for computing a hypothesis with the heuristic search)
#D: Predictive accuracy
#   (If the hypothesis can explain the test example, then D = 100; otherwise 0)
#E: The average of the predictive accuracy among the hypotheses 
#   (This is useful only for using CF-induction with the enumeration mode)

#A,#B,#C,#D,#E 
0.2,621,517,0,0
0.2,504,918,0,0
0.2,651,473,0,0
0.2,542,533,0,0
0.2,507,528,100,100
0.2,673,428,0,0
0.2,722,517,0,0
0.2,532,684,100,100
0.2,713,505,0,0
0.2,594,560,0,0
0.2,581,542,0,0
0.2,589,453,0,0
0.2,522,505,100,100
0.2,564,437,0,0
0.2,643,525,100,100
0.2,636,566,100,100
0.2,694,505,100,100
0.2,608,543,0,0
0.2,543,446,0,0
0.2,613,561,100,100
0.3,604,506,0,0
0.3,820,496,0,0
0.3,563,528,0,0
0.3,560,552,100,100
0.3,588,451,0,0
0.3,587,470,100,100
0.3,684,616,0,0
0.3,712,547,100,100
0.3,634,506,100,100
0.3,606,583,100,100
0.3,708,553,0,0
0.3,666,604,0,0
0.3,660,598,100,100
0.3,617,524,0,0
0.3,653,519,100,100
0.3,604,531,0,0
0.3,776,465,100,100
0.3,599,468,100,100
0.3,645,500,0,0
0.3,628,541,0,0
0.4,690,485,100,100
0.4,905,538,100,100
0.4,635,686,0,0
0.4,751,487,0,0
0.4,789,580,100,100
0.4,712,641,100,100
0.4,766,623,100,100
0.4,896,639,100,100
0.4,583,698,0,0
0.4,633,624,100,100
0.4,645,572,100,100
0.4,634,612,100,100
0.4,735,573,100,100
0.4,699,688,100,100
0.4,644,636,100,100
0.4,630,690,0,0
0.4,675,592,100,100
0.4,855,547,0,0
0.4,632,507,0,0
0.4,756,486,100,100
0.5,759,666,100,100
0.5,709,624,100,100
0.5,806,565,100,100
0.5,646,537,0,0
0.5,878,561,0,0
0.5,617,534,100,100
0.5,640,578,100,100
0.5,627,596,100,100
0.5,707,738,100,100
0.5,782,617,100,100
0.5,712,576,0,0
0.5,781,701,0,0
0.5,636,661,100,100
0.5,884,593,100,100
0.5,875,898,100,100
0.5,661,640,100,100
0.5,590,556,0,0
0.5,658,601,0,0
0.5,763,581,100,100
0.5,649,620,0,0
0.6,774,751,0,0
0.6,653,627,100,100
0.6,683,726,100,100
0.6,665,582,100,100
0.6,659,768,100,100
0.6,602,911,100,100
0.6,851,853,100,100
0.6,668,876,100,100
0.6,677,696,100,100
0.6,731,743,100,100
0.6,830,582,0,0
0.6,720,648,100,100
0.6,613,667,100,100
0.6,660,595,0,0
0.6,746,722,100,100
0.6,755,625,100,100
0.6,755,614,100,100
0.6,733,616,100,100
0.6,785,530,100,100
0.6,727,702,100,100
0.7,726,757,100,100
0.7,757,859,0,0
0.7,795,820,100,100
0.7,757,746,0,0
0.7,658,781,100,100
0.7,634,750,100,100
0.7,712,737,100,100
0.7,743,740,100,100
0.7,696,861,100,100
0.7,703,861,100,100
0.7,711,721,100,100
0.7,664,959,100,100
0.7,840,864,0,0
0.7,771,729,100,100
0.7,663,652,100,100
0.7,653,704,100,100
0.7,880,759,100,100
0.7,728,786,100,100
0.7,663,768,100,100
0.7,645,663,100,100
0.8,542,761,0,0
0.8,614,738,100,100
0.8,699,737,100,100
0.8,844,708,100,100
0.8,1217,859,0,0
0.8,860,1075,100,100
0.8,686,834,100,100
0.8,950,1053,100,100
0.8,660,787,100,100
0.8,641,873,100,100
0.8,614,747,100,100
0.8,777,909,100,100
0.8,938,741,100,100
0.8,726,879,100,100
0.8,770,870,100,100
0.8,776,788,100,100
0.8,718,913,100,100
0.8,797,1628,100,100
0.8,781,844,100,100
0.8,709,823,100,100
0.9,714,899,100,100
0.9,782,755,0,0
0.9,980,897,100,100
0.9,779,733,100,100
0.9,782,996,100,100
0.9,660,949,100,100
0.9,799,941,100,100
0.9,896,1207,100,100
0.9,674,914,100,100
0.9,693,859,100,100
0.9,769,779,100,100
0.9,830,914,100,100
0.9,736,876,100,100
0.9,854,862,0,0
0.9,737,938,100,100
0.9,569,908,100,100
0.9,697,918,100,100
0.9,791,944,100,100
0.9,775,951,0,0
0.9,528,915,0,0
1.0,644,879,100,100
1.0,627,867,100,100
1.0,782,963,100,100
1.0,867,905,100,100
1.0,801,945,100,100
1.0,702,910,100,100
1.0,701,935,100,100
1.0,748,897,100,100
1.0,801,4176,100,100
1.0,723,5668,100,100
1.0,736,902,100,100
1.0,783,939,0,0
1.0,752,912,100,100
1.0,703,5598,100,100
1.0,844,976,100,100
1.0,849,884,100,100
1.0,826,926,100,100
1.0,793,968,100,100
1.0,924,5615,100,100
1.0,715,7252,100,100
